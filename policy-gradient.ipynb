{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Let's start by creating the blackjack environment.\n",
    "\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "# Whether to follow the exact rules outlined in the book by Sutton and Barto. If `sab` is `True`, the keyword argument `natural` will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to get the first observation\n",
    "done = False\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random action from all valid actions\n",
    "action = env.action_space.sample()\n",
    "# action=1\n",
    "\n",
    "# execute the action in our environment and receive infos from the environment\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# observation=(24, 10, False)\n",
    "# reward=-1.0\n",
    "# terminated=True\n",
    "# truncated=False\n",
    "# info={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Policy-Gradient Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent: \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
